{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This notebook represents a summary of the assignment1 and includes the knn, svm, softmax, two layer net classifiers with the best results achieved on the classification problem for CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Similar to previous exercises, we will load CIFAR-10 data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data type:  <type 'numpy.ndarray'>\n",
      "Training data shape:  (50000, 32, 32, 3)\n",
      "Training labels shape:  (50000,)\n",
      "Test data type:  <type 'numpy.ndarray'>\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.features import color_histogram_hsv, hog_feature\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_CIFAR10_data()\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data type: ', type(X_train))\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data type: ', type(X_test))\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw pixels as image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_raw_pixels shape:  (50000, 3073)\n",
      "X_test_raw_pixels shape:  (10000, 3073)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: reshape the image data into rows\n",
    "X_train_raw_pixels = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_test_raw_pixels = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "# Preprocessing: subtract the mean image\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train_raw_pixels, axis=0)\n",
    "\n",
    "# second: subtract the mean image from train and test data\n",
    "X_train_raw_pixels -= mean_image\n",
    "X_test_raw_pixels -= mean_image\n",
    "\n",
    "# third: append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train_raw_pixels = np.hstack([X_train_raw_pixels, np.ones((X_train_raw_pixels.shape[0], 1))])\n",
    "X_test_raw_pixels = np.hstack([X_test_raw_pixels, np.ones((X_test_raw_pixels.shape[0], 1))])\n",
    "\n",
    "# As a sanity check, print out the shapes of the data\n",
    "print('X_train_raw_pixels shape: ', X_train_raw_pixels.shape)\n",
    "print('X_test_raw_pixels shape: ', X_test_raw_pixels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract image features (HOG + hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done extracting features for 1000 / 50000 images\n",
      "Done extracting features for 2000 / 50000 images\n",
      "Done extracting features for 3000 / 50000 images\n",
      "Done extracting features for 4000 / 50000 images\n",
      "Done extracting features for 5000 / 50000 images\n",
      "Done extracting features for 6000 / 50000 images\n",
      "Done extracting features for 7000 / 50000 images\n",
      "Done extracting features for 8000 / 50000 images\n",
      "Done extracting features for 9000 / 50000 images\n",
      "Done extracting features for 10000 / 50000 images\n",
      "Done extracting features for 11000 / 50000 images\n",
      "Done extracting features for 12000 / 50000 images\n",
      "Done extracting features for 13000 / 50000 images\n",
      "Done extracting features for 14000 / 50000 images\n",
      "Done extracting features for 15000 / 50000 images\n",
      "Done extracting features for 16000 / 50000 images\n",
      "Done extracting features for 17000 / 50000 images\n",
      "Done extracting features for 18000 / 50000 images\n",
      "Done extracting features for 19000 / 50000 images\n",
      "Done extracting features for 20000 / 50000 images\n",
      "Done extracting features for 21000 / 50000 images\n",
      "Done extracting features for 22000 / 50000 images\n",
      "Done extracting features for 23000 / 50000 images\n",
      "Done extracting features for 24000 / 50000 images\n",
      "Done extracting features for 25000 / 50000 images\n",
      "Done extracting features for 26000 / 50000 images\n",
      "Done extracting features for 27000 / 50000 images\n",
      "Done extracting features for 28000 / 50000 images\n",
      "Done extracting features for 29000 / 50000 images\n",
      "Done extracting features for 30000 / 50000 images\n",
      "Done extracting features for 31000 / 50000 images\n",
      "Done extracting features for 32000 / 50000 images\n",
      "Done extracting features for 33000 / 50000 images\n",
      "Done extracting features for 34000 / 50000 images\n",
      "Done extracting features for 35000 / 50000 images\n",
      "Done extracting features for 36000 / 50000 images\n",
      "Done extracting features for 37000 / 50000 images\n",
      "Done extracting features for 38000 / 50000 images\n",
      "Done extracting features for 39000 / 50000 images\n",
      "Done extracting features for 40000 / 50000 images\n",
      "Done extracting features for 41000 / 50000 images\n",
      "Done extracting features for 42000 / 50000 images\n",
      "Done extracting features for 43000 / 50000 images\n",
      "Done extracting features for 44000 / 50000 images\n",
      "Done extracting features for 45000 / 50000 images\n",
      "Done extracting features for 46000 / 50000 images\n",
      "Done extracting features for 47000 / 50000 images\n",
      "Done extracting features for 48000 / 50000 images\n",
      "Done extracting features for 49000 / 50000 images\n"
     ]
    }
   ],
   "source": [
    "from cs231n.features import *\n",
    "\n",
    "num_color_bins = 10 # Number of bins in the color histogram\n",
    "feature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\n",
    "X_train_feats = extract_features(X_train, feature_fns, verbose=True)\n",
    "X_test_feats = extract_features(X_test, feature_fns)\n",
    "\n",
    "# Preprocessing: Subtract the mean feature\n",
    "mean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats -= mean_feat\n",
    "X_test_feats -= mean_feat\n",
    "\n",
    "# Preprocessing: Divide by standard deviation. This ensures that each feature\n",
    "# has roughly the same scale.\n",
    "std_feat = np.std(X_train_feats, axis=0, keepdims=True)\n",
    "X_train_feats /= std_feat\n",
    "X_test_feats /= std_feat\n",
    "\n",
    "# Preprocessing: Add a bias dimension\n",
    "X_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\n",
    "X_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cs231n.classifiers import KNearestNeighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN classifier (Image features: raw pixels)\n",
      "best_k: 10\n",
      "test accuracy: 0.338600\n"
     ]
    }
   ],
   "source": [
    "# kNN classifier\n",
    "# Image features: raw pixels\n",
    "best_k = 10\n",
    "\n",
    "knn = KNearestNeighbor()\n",
    "knn.train(X_train_raw_pixels, y_train)\n",
    "test_acc = np.mean(knn.predict(X_test_raw_pixels, k=best_k) == y_test)\n",
    "\n",
    "print('kNN classifier (Image features: raw pixels)')\n",
    "print('best_k: %d' % (best_k))\n",
    "print('test accuracy: %f' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN classifier (Image features: HOG + hue)\n",
      "best_k: 13\n",
      "test accuracy: 0.443100\n"
     ]
    }
   ],
   "source": [
    "# kNN classifier\n",
    "# Image features: HOG + hue\n",
    "best_k = 13\n",
    "\n",
    "knn = KNearestNeighbor()\n",
    "knn.train(X_train_feats, y_train)\n",
    "test_acc = np.mean(knn.predict(X_test_feats, k=best_k) == y_test)\n",
    "\n",
    "print('kNN classifier (Image features: HOG + hue)')\n",
    "print('best_k: %d' % (best_k))\n",
    "print('test accuracy: %f' % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cs231n.classifiers import Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax classifier (Image features: raw pixels)\n",
      "lr: 5.000000e-07\n",
      "reg: 3.214286e+04\n",
      "batch_size: 200\n",
      "num_iters: 1500\n",
      "training accuracy: 0.315240\n",
      "test accuracy: 0.320800\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "# Image features: raw pixels\n",
    "np.random.seed(0)\n",
    "\n",
    "lr = 5e-07\n",
    "reg = 3.214286e+04\n",
    "batch_size = 200\n",
    "num_iters = 1500\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax.train(X_train_raw_pixels, y_train, \n",
    "              learning_rate=lr, reg=reg, \n",
    "              num_iters=1500)\n",
    "        \n",
    "train_acc = np.mean(softmax.predict(X_train_raw_pixels) == y_train)\n",
    "test_acc = np.mean(softmax.predict(X_test_raw_pixels) == y_test)\n",
    "\n",
    "print('Softmax classifier (Image features: raw pixels)')\n",
    "print('lr: %e' % (lr))\n",
    "print('reg: %e' % (reg))\n",
    "print('batch_size: %d' % (batch_size))\n",
    "print('num_iters: %d' % (num_iters))\n",
    "print('training accuracy: %f' % (train_acc))\n",
    "print('test accuracy: %f' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax classifier (Image features: HOG + hue)\n",
      "lr: 5.000000e-07\n",
      "reg: 1.000000e+04\n",
      "batch_size: 200\n",
      "num_iters: 1500\n",
      "training accuracy: 0.415340\n",
      "test accuracy: 0.409300\n"
     ]
    }
   ],
   "source": [
    "# Softmax\n",
    "# Image features: HOG + hue\n",
    "np.random.seed(0)\n",
    "\n",
    "lr = 5e-07\n",
    "reg = 1e+04\n",
    "batch_size = 200\n",
    "num_iters = 1500\n",
    "\n",
    "softmax = Softmax()\n",
    "softmax.train(X_train_feats, y_train, \n",
    "              learning_rate=lr, reg=reg, \n",
    "              num_iters=1500)\n",
    "\n",
    "train_acc = np.mean(softmax.predict(X_train_feats) == y_train)\n",
    "test_acc = np.mean(softmax.predict(X_test_feats) == y_test)\n",
    "\n",
    "print('Softmax classifier (Image features: HOG + hue)')\n",
    "print('lr: %e' % (lr))\n",
    "print('reg: %e' % (reg))\n",
    "print('batch_size: %d' % (batch_size))\n",
    "print('num_iters: %d' % (num_iters))\n",
    "print('training accuracy: %f' % (train_acc))\n",
    "print('test accuracy: %f' % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cs231n.classifiers.linear_classifier import LinearSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Image features: raw pixels)\n",
      "lr: 1.258925e-07\n",
      "reg: 1.467799e+04\n",
      "batch_size: 2000\n",
      "num_iters: 1500\n",
      "training accuracy: 0.384380\n",
      "test accuracy: 0.376200\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "# Image features: raw pixels\n",
    "np.random.seed(0)\n",
    "\n",
    "lr = 1.258925e-07\n",
    "reg = 1.467799e+04\n",
    "batch_size = 2000\n",
    "num_iters = 1500\n",
    "\n",
    "svm = LinearSVM()\n",
    "svm.train(X_train_raw_pixels, y_train, \n",
    "          learning_rate=lr, reg=reg, \n",
    "          batch_size=batch_size,\n",
    "          num_iters=num_iters)\n",
    "\n",
    "train_acc = np.mean(svm.predict(X_train_raw_pixels) == y_train)\n",
    "test_acc = np.mean(svm.predict(X_test_raw_pixels) == y_test)\n",
    "\n",
    "print('SVM (Image features: raw pixels)')\n",
    "print('lr: %e' % (lr))\n",
    "print('reg: %e' % (reg))\n",
    "print('batch_size: %d' % (batch_size))\n",
    "print('num_iters: %d' % (num_iters))\n",
    "print('training accuracy: %f' % (train_acc))\n",
    "print('test accuracy: %f' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Image features: HOG + hue)\n",
      "lr: 1.000000e-08\n",
      "reg: 1.000000e+06\n",
      "batch_size: 200\n",
      "num_iters: 3000\n",
      "training accuracy: 0.415040\n",
      "test accuracy: 0.411100\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "# Image features: HOG + hue\n",
    "np.random.seed(0)\n",
    "\n",
    "lr = 1e-08\n",
    "reg = 1e+06\n",
    "batch_size = 200\n",
    "num_iters = 3000\n",
    "\n",
    "svm = LinearSVM()\n",
    "svm.train(X_train_feats, y_train, \n",
    "          learning_rate=lr, reg=reg, \n",
    "          batch_size=batch_size,\n",
    "          num_iters=num_iters)\n",
    "\n",
    "train_acc = np.mean(svm.predict(X_train_feats) == y_train)\n",
    "test_acc = np.mean(svm.predict(X_test_feats) == y_test)\n",
    "\n",
    "print('SVM (Image features: HOG + hue)')\n",
    "print('lr: %e' % (lr))\n",
    "print('reg: %e' % (reg))\n",
    "print('batch_size: %d' % (batch_size))\n",
    "print('num_iters: %d' % (num_iters))\n",
    "print('training accuracy: %f' % (train_acc))\n",
    "print('test accuracy: %f' % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cs231n.classifiers.neural_net import TwoLayerNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3073)\n",
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: Remove the bias dimension\n",
    "# Make sure to run this cell only ONCE\n",
    "print(X_train_raw_pixels.shape)\n",
    "X_train_raw_pixels = X_train_raw_pixels[:, :-1]\n",
    "X_test_raw_pixels = X_test_raw_pixels[:, :-1]\n",
    "\n",
    "print(X_train_raw_pixels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 155)\n",
      "(50000, 154)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: Remove the bias dimension\n",
    "# Make sure to run this cell only ONCE\n",
    "print(X_train_feats.shape)\n",
    "X_train_feats = X_train_feats[:, :-1]\n",
    "X_test_feats = X_test_feats[:, :-1]\n",
    "\n",
    "print(X_train_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two layer net (Image features: raw pixels)\n",
      "hidden_dim: 100\n",
      "lr_decay: 0.950000\n",
      "lr: 1.000000e-03\n",
      "reg: 1.000000e+00\n",
      "batch_size: 200\n",
      "num_iters: 50000\n",
      "training accuracy: 0.554100\n",
      "test accuracy: 0.517100\n"
     ]
    }
   ],
   "source": [
    "# Two layer net\n",
    "# Image features: raw pixels\n",
    "np.random.seed(0)\n",
    "\n",
    "input_dim = X_train_raw_pixels.shape[1]\n",
    "num_classes = 10\n",
    "hidden_dim = 100\n",
    "lr_decay = 0.95\n",
    "lr = 1e-03\n",
    "reg = 1\n",
    "batch_size = 200\n",
    "num_iters = 50 * 1000\n",
    "\n",
    "net = TwoLayerNet(input_dim, hidden_dim, num_classes)\n",
    "net.train(X_train_raw_pixels, y_train, \n",
    "          X_test_raw_pixels[range(50)], y_test[range(50)],\n",
    "          num_iters=num_iters,\n",
    "          batch_size=batch_size,\n",
    "          learning_rate=lr, \n",
    "          learning_rate_decay=lr_decay,\n",
    "          reg=reg, verbose=False)\n",
    "            \n",
    "train_acc = np.mean(net.predict(X_train_raw_pixels) == y_train)\n",
    "test_acc = np.mean(net.predict(X_test_raw_pixels) == y_test)\n",
    "            \n",
    "print('Two layer net (Image features: raw pixels)')\n",
    "print('hidden_dim: %d' % (hidden_dim))\n",
    "print('lr_decay: %f' % (lr_decay))\n",
    "print('lr: %e' % (lr))\n",
    "print('reg: %e' % (reg))\n",
    "print('batch_size: %d' % (batch_size))\n",
    "print('num_iters: %d' % (num_iters))\n",
    "print('training accuracy: %f' % (train_acc))\n",
    "print('test accuracy: %f' % (test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two layer net (Image features: HOG + hue)\n",
      "hidden_dim: 100\n",
      "lr_decay: 0.950000\n",
      "lr: 1.000000e-01\n",
      "reg: 1.584893e-11\n",
      "batch_size: 200\n",
      "num_iters: 15000\n",
      "training accuracy: 0.648560\n",
      "test accuracy: 0.578300\n"
     ]
    }
   ],
   "source": [
    "# Two layer net\n",
    "# Image features: HOG + hue\n",
    "np.random.seed(0)\n",
    "\n",
    "input_dim = X_train_feats.shape[1]\n",
    "num_classes = 10\n",
    "hidden_dim = 100\n",
    "lr_decay = 0.95\n",
    "lr = 0.1\n",
    "reg = 1.58489319246e-11\n",
    "batch_size = 200\n",
    "num_iters = 15 * 1000\n",
    "\n",
    "net = TwoLayerNet(input_dim, hidden_dim, num_classes)\n",
    "net.train(X_train_feats, y_train, \n",
    "          X_test_feats[range(50)], y_test[range(50)],\n",
    "          num_iters=num_iters,\n",
    "          batch_size=batch_size,\n",
    "          learning_rate=lr, \n",
    "          learning_rate_decay=lr_decay,\n",
    "          reg=reg, verbose=False)\n",
    "            \n",
    "train_acc = np.mean(net.predict(X_train_feats) == y_train)\n",
    "test_acc = np.mean(net.predict(X_test_feats) == y_test)\n",
    "            \n",
    "print('Two layer net (Image features: HOG + hue)')\n",
    "print('hidden_dim: %d' % (hidden_dim))\n",
    "print('lr_decay: %f' % (lr_decay))\n",
    "print('lr: %e' % (lr))\n",
    "print('reg: %e' % (reg))\n",
    "print('batch_size: %d' % (batch_size))\n",
    "print('num_iters: %d' % (num_iters))\n",
    "print('training accuracy: %f' % (train_acc))\n",
    "print('test accuracy: %f' % (test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final results\n",
    "| Classifier | Features | Learning rate | Regularization strength | Batch size | Number of iterations | Other hyperparameters | Training accuracy | Test accuracy |  \n",
    "| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| kNN | raw pixels | - | - | - | - | k = 10 | - | 33.86% |\n",
    "| kNN | HOG + hue  | - | - | - | - | k = 13 | - | 44.31% |\n",
    "| Softmax | raw pixels | 5.000000e-07 | 3.214286e+04 | 200 | 1500 | - | 31.52% | 32.08% |\n",
    "| Softmax | HOG + hue  | 5.000000e-07 | 1.000000e+04 | 200 | 1500| - | 41.53% | 40.93% |\n",
    "| SVM | raw pixels | 1.258925e-07 | 1.467799e+04 | 2000 | 1500 | - | 38.43% | 37.62% |\n",
    "| SVM | HOG + hue  | 1.000000e-08 | 1.000000e+06 | 200 | 3000 | - | 41.50% | 41.11% |\n",
    "| Two layer net | raw pixels | 1.000000e-03 | 1.000000e+00 | 200 | 50000 | lr_decay: 0.95, hidden_dim: 100| 55.41% | 51.71% |\n",
    "| Two layer net | HOG + hue | 1.000000e-01 | 1.584893e-11 | 200 | 15000 | lr_decay: 0.95, hidden_dim: 100 | 64.85% | **57.83%** |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
